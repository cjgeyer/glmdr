
\documentclass[11pt]{article}

\usepackage{indentfirst}
\usepackage{natbib}
\usepackage{url}

\newcommand{\REVISED}{\begin{center}\LARGE REVISED DOWN TO HERE\end{center}}

%\VignetteEngine{knitr::knitr}

\begin{document}

\title{Polishing GLM Fits}

\author{Charles J. Geyer}

\maketitle

<<options-width,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
@

\section{Introduction}

We want reported MLE and Fisher information better than what R
function \texttt{glm} outputs with its default tolerance.
We do not believe its computations are sufficiently accurate
(coping well with inexactness of computer arithmetic) to set its
tolerance very close to zero.

We will do better on our own.
R function \verb@glmdr:::make.mlogl@ computes accurately minus
the log likelihood
and two derivatives avoiding overflow, underflow, catastrophic cancellation,
and other problems of inexact computer arithmetic.

\section{R}

<<libraries>>=
library(glmdr, lib.loc = "../package/glmdr.Rcheck")
@

\begin{itemize}
\item The version of R used to make this document is \Sexpr{getRversion()}.
\item The version of the \texttt{glmdr} package used to make this document is
    \Sexpr{packageVersion("glmdr")}.
\item The version of the \texttt{knitr} package used to make this document is
    \Sexpr{packageVersion("knitr")}.
\end{itemize}

\section{Example I} \label{sec:ex-i}

\subsection{Fit Using R Function \texttt{glm}}

This is the complete separation example of Agresti (see help for this dataset
for source).
<<example-i-glm>>=
data(complete)
gout <- glm(y ~ x, family = "binomial", data = complete,
    x = TRUE)
summary(gout)
@
% compares OK with 8931 handout infinity.pdf

\subsection{A Useful Function}

Define the inverse logit function with careful computation that
avoids overflow, underflow, catastrophic cancellation, and other
issues with inexact computer arithmetic.
<<invlogit>>=
invlogit <- function(theta)
    ifelse(theta < 0,
        exp(theta) / (1 + exp(theta)),
        1 / (1 + exp(- theta)))
@

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-i-glm-fish>>=
# extract model matrix, response vector, and offset vector
modmat <- gout$x
mf <- model.frame(gout)
resp <- model.response(mf)
offs <- model.offset(mf)

# calculate minus log likelihood and two derivatives
mlogl <- glmdr:::make.mlogl(modmat, resp, offs, "binomial")
mout <- mlogl(coefficients(gout))
@

Check that hessian is correct.
<<example-i-check-fish>>=
theta.glm <- predict(gout)
p.glm <- invlogit(theta.glm)
q.glm <- invlogit(- theta.glm)
# only good for Bernoulli (zero-or-one-valued) response
fish.glm <- t(modmat) %*% diag(p.glm * q.glm) %*% modmat
all.equal(mout$hessian, fish.glm, check.attributes = FALSE)
@

Look at eigenvalues of Fisher information matrix at the MLE found by
R function \texttt{glm}.
<<example-i-fish-eigen>>=
eout <- eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)
eout$values
@

Compare to default tolerance for R function \texttt{all.equal}.
<<example-i-how-small>>=
foo <- args(getS3method("all.equal", "numeric"))
foo
all.equal.default.tolerance <- eval(as.list(foo)$tolerance)
all.equal.default.tolerance
@

According to R function \texttt{all.equal} with its default
tolerance we do not have both
eigenvalues equal to zero.
<<example-i-eigenvalues-zero>>=
eout$values < all.equal.default.tolerance
@
Of course, we could change the tolerance, but to what?
We need something that works for all problems not just this one.
You need to already know the answer to choose the tolerance to
get the correct answer.  So that's no help.

\subsection{Newton Step}

Find Newton step.
<<example-i-newt>>=
newt.step <- solve(mout$hessian, - mout$gradient)
newt.step
@

\subsection{Direction of Recession}

Could the Newton direction possibly be a direction of recession (DOR)?

Map to saturated model canonical parameter space.
<<example-i-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
newt.step.sat
@

This is obviously a DOR for this toy problem.  In general, we need code
to check.  The check here only works for Bernoulli regression, not
general binomial regression.  We also do not deal with zero components
(possibly computed inexactly) of \texttt{newt.step.sat}.
<<example-i-dor-check>>=
all(ifelse(newt.step.sat < 0, resp == 0,
    ifelse(newt.step.sat > 0, resp == 1, TRUE)))
@

We will refine this check in examples below.

\subsection{Limiting Conditional Model}

For this example,
we have found a DOR and hence need to take the limit in this direction.
Since we are not using a computer algebra system, like Mathematica
or Maple, we cannot actually take the limit.  Let us just take a
step that is 100 times the Newton step.

<<example-i-beta-too>>=
beta.too <- coefficients(gout) + 100 * newt.step
@

\pagebreak[3]
And redo the Fisher information calculation.
<<example-i-fish-lcm>>=
mout <- mlogl(beta.too)
eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)$values
@

Now there is no question about the eigenvalues being both zero
modulo inexactness of computer arithmetic.

<<example-i-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <-
    max(eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)$values)
@

\subsection{Conclusion}

The limiting conditional model (LCM) is completely degenerate.
It has no parameters to estimate.  We are done with this example.

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}
and the tolerance.
<<example-i-clean>>=
rm(list = setdiff(ls(),
    c("invlogit", "all.equal.default.tolerance")))
@

\section{Example II}

\subsection{Fit Using R Function \texttt{glm}}

This is the quasi-complete separation example of Agresti
(see help for this dataset for source).
<<example-ii-glm>>=
data(quasi)
gout <- glm(y ~ x, family = "binomial", data = quasi, x = TRUE)
summary(gout)
@
% compares OK with 8931 handout infinity.pdf

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-ii-glm-fish>>=
<<example-i-glm-fish>>
@

Check hessian is correct.
<<example-ii-check-fish>>=
<<example-i-check-fish>>
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-ii-fish-eigen>>=
<<example-i-fish-eigen>>
@

Now comparing to default tolerance for R function \texttt{all.equal}
<<example-ii-eigenvalues-zero>>=
<<example-i-eigenvalues-zero>>
@
\noindent
does give the correct number of eigenvalues that are zero
modulo inexactness of computer arithmetic (we know this toy problem
was constructed to have one null eigenvalue of Fisher information).

\subsection{Newton Step}

Find Newton step.
<<example-ii-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

Could the Newton direction possibly be a DOR?
Because its components are not exactly round numbers, we suspect
it is not exactly a DOR.

Map to saturated model canonical parameter space.
<<example-ii-newt-sat>>=
<<example-i-newt-sat>>
@

This is not exactly a DOR.
<<example-ii-dor-check>>=
is.DOR.bernoulli <- function(theta)
    all(ifelse(theta < 0, resp == 0, ifelse(theta > 0, resp == 1, TRUE)))
is.DOR.bernoulli(newt.step.sat)
@

But it is approximately.
<<example-ii-newt-sat-zap>>=
is.DOR.bernoulli(zapsmall(newt.step.sat))
@

But this is not much help.  Since it is not \emph{exactly} a DOR,
we cannot take the limit in this direction.  Since it is \emph{approximately}
a DOR, we can probably go uphill on the log likelihood in this direction.
But we don't know how far.

\subsection{Line Search}

So we do a line search in the line along the Newton step.
<<example-ii-line-search>>=
linesearchfun <- function(s) {
    mout <- mlogl(coefficients(gout) + s * newt.step)
    sum(mout$gradient * newt.step)
}
Vectorize(linesearchfun)(0:32)
@

What happens when $s$ gets large?  This doesn't look smooth.  Something
happens around $s = 18$.  Let's do a careful calculation for $s = 20$.
<<example-ii-line-search-debug>>=
beta.debug <- coefficients(gout) + 20 * newt.step
beta.debug
theta.debug <- as.vector(modmat %*% beta.debug)
theta.debug
@

We see we get unavoidable catastrophic cancellation for the last two
components of this vector.
<<example-ii-line-search-debug-cont>>=
p.debug <- invlogit(theta.debug)
q.debug <- invlogit(- theta.debug)
p.debug
q.debug
g.debug <- ifelse(resp == 0, - p.debug, q.debug)
g.debug
@

Above is the gradient with respect to the saturated model canonical
parameters.  Now with respect to the submodel canonical parameters.
<<example-ii-line-search-debug-cont-too>>=
as.numeric(t(modmat) %*% g.debug)
@

And then finally the dot product of this with respect to the Newton step.
<<example-ii-line-search-debug-cont-too-too>>=
sum(as.numeric(t(modmat) %*% g.debug) * newt.step)
@

And we see that we had catastrophic cancellation adding
<<example-ii-line-search-debug-cont-too-too-tutu>>=
as.numeric(t(modmat) %*% g.debug) * newt.step
@

So unless we have a completely degenerate LCM, we will get some
catastrophic cancellation in calculating this line search (at some
point).

It looks like 16 Newton steps is safe.
<<example-ii-beta-too>>=
beta.too <- coefficients(gout) + 16 * newt.step
@

And redo the Fisher information calculation.
<<example-ii-fish-lcm>>=
mout <- mlogl(beta.too)
eout <- eigen(mout$hessian, symmetric = TRUE)
eout$values
@

Now it is more clear that there is one zero eigenvalue
modulo inexactness of computer arithmetic.

<<example-ii-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

\subsection{Limiting Conditional Model}

We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-ii-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
nulls

nulls.sat <- modmat %*% nulls
nulls.sat
nulls.sat == 0
@

Here we have actually calculated the linearity exactly,
but, in general, we would have to worry about inexactness of computer
arithmetic.  We also deal here only with the special case that there
is only one null eigenvector.  We will generalize this in other examples.

<<example-ii-linearity>>=
linearity <- as.vector(nulls.sat == 0)
linearity
@

\pagebreak[3]
Fit the LCM.
<<example-ii-glm-lcm>>=
gout.lcm <- glm(y ~ x, family = "binomial", data = quasi,
    subset = linearity)
summary(gout.lcm)
@
% compares OK with 8931 handout infinity.pdf

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-ii-glm-vcov>>=
vcov(gout.lcm)
@

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}
and the tolerance.
<<example-ii-clean>>=
<<example-i-clean>>
@

\section{Example III}

\subsection{Fit Using R Function \texttt{glm}}

This is the complete separation example of Geyer
(see help for this dataset for source).
<<example-iii-glm>>=
data(quadratic)
gout <- glm(y ~ x + I(x^2), family = "binomial",
    data = quadratic, x = TRUE)
summary(gout)
@
% technical report 672, file phaseTR.pdf
% does not have this output

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-iii-glm-fish>>=
<<example-i-glm-fish>>
@

Check hessian is correct.
<<example-iii-check-fish>>=
<<example-i-check-fish>>=
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-iii-fish-eigen>>=
<<example-i-fish-eigen>>
@

Now comparing to default tolerance for R function \texttt{all.equal}
<<example-iii-eigenvalues-zero>>=
<<example-i-eigenvalues-zero>>
@
\noindent
does \emph{not} give the correct number of eigenvalues that are zero
modulo inexactness of computer arithmetic (we know this toy problem
was constructed to have completely degenerate LCM).

\subsection{Newton Step}

Find Newton step.
<<example-iii-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

Could the Newton direction possibly be a DOR?

Map to saturated model canonical parameter space.
<<example-iii-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
data.frame(quadratic, newt.step.sat)
@

This is obviously a DOR for this toy problem.
As in Section~\ref{sec:ex-i} above, we do a non-general check,
because that is good enough when the LCM is completely degenerate.
<<example-iii-dor-check>>=
<<example-i-dor-check>>
@

\subsection{Limiting Conditional Model}

Also following Section~\ref{sec:ex-i} above,
we take a step that is 100 times the Newton step.

<<example-iii-beta-too>>=
<<example-i-beta-too>>
@

And redo the Fisher information calculation.
<<example-iii-fish-lcm>>=
<<example-i-fish-lcm>>
@

<<example-iii-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)$values))
@

Now there is no question about the eigenvalues being all zero
modulo inexactness of computer arithmetic.

\subsection{Conclusion}

The limiting conditional model (LCM) is completely degenerate.
It has no parameters to estimate.  We are done with this example.

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}
and the tolerance.
<<example-iii-clean>>=
<<example-i-clean>>
@

\section{Example IV}

\subsection{Fit Using R Function \texttt{glm}}

This is the categorical data example of Geyer
(see help for this dataset for source).
<<example-iv-glm>>=
data(catrec)
gout <- glm(y ~ (.)^3, family = "poisson", data = catrec, x = TRUE)
summary(gout)
@
% technical report 672, file phaseTR.pdf
% does not have this output

Note that, unlike the preceding examples, R function \texttt{glm}
gives no warning of solutions at infinity.  So this is a false negative
for those warnings.

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-iv-glm-fish>>=
# extract model matrix, response vector, and offset vector
modmat <- gout$x
mf <- model.frame(gout)
resp <- model.response(mf)
offs <- model.offset(mf)

# calculate minus log likelihood and two derivatives
mlogl <- glmdr:::make.mlogl(modmat, resp, offs, "poisson")
mout <- mlogl(coefficients(gout))
@

Check hessian is correct.
<<example-iv-check-fish>>=
theta.glm <- predict(gout)
fish.glm <- t(modmat) %*% diag(exp(theta.glm)) %*% modmat
all.equal(mout$hessian, fish.glm, check.attributes = FALSE)
@

\pagebreak[3]
Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-iv-fish-eigen>>=
<<example-i-fish-eigen>>
@

\subsection{Newton Step}

Find Newton step.
<<example-iv-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

Could the Newton direction possibly be a DOR?
Because its components are not exactly round numbers, we suspect
it is not exactly a DOR.

Map to saturated model canonical parameter space.
<<example-iv-newt-sat>>=
<<example-ii-newt-sat>>
@

This is not exactly a DOR.
<<example-iv-dor-check>>=
is.DOR.poisson <- function(theta)
    all(ifelse(theta < 0, resp == 0, ifelse(theta > 0, resp == Inf, TRUE)))
is.DOR.poisson(newt.step.sat)
@

But it is approximately.
<<example-iv-newt-sat-zap>>=
is.DOR.poisson(zapsmall(newt.step.sat))
@

But this is not much help. 

\subsection{Line Search}

So we do a line search in the line along the Newton step.
<<example-iv-line-search>>=
linesearchfun <- function(s) {
    mout <- mlogl(coefficients(gout) + s * newt.step)
    sum(mout$gradient * newt.step)
}
foo <- Vectorize(linesearchfun)(0:32)
foo
@

As we saw in Example~II, there is something funny that happens when
the result (derivative of minus the log likelihood restricted to the
line along the Newton step) becomes about the machine epsilon.

In particular, we see this fluctuates in sign (as calculated) but
theoretically cannot.  The derivative of a strictly convex function
is a strictly increasing function.
<<example-iv-line-search-diff>>=
diff(foo)
@

Every minus sign above has to be rounding error.  Let us take the path
up to (but not including) the first positive result or the first negative
difference of results.
<<example-iv-beta-too>>=
foo.good <- foo < 0 & c(Inf, diff(foo)) > 0
ifoo <- min(which(! foo.good)) - 1
ifoo
beta.too <- coefficients(gout) + ifoo * newt.step
@

And redo the Fisher information calculation.
<<example-iv-fish-lcm>>=
<<example-ii-fish-lcm>>
@

Now it is more clear that there is one zero eigenvalue
modulo inexactness of computer arithmetic.
We cannot really expect more accuracy than
<<example-iv-fish-lcm-eigen-accuracy>>=
max(eout$values) * .Machine$double.eps
@
\noindent

<<example-iv-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

\subsection{Limiting Conditional Model}
We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-iv-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
# is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
# nulls

nulls.sat <- modmat %*% nulls
# nulls.sat
any(nulls.sat == 0)
any(abs(nulls.sat) < all.equal.default.tolerance)
@

Here we have \emph{not} calculated the linearity exactly,
so we have to worry about inexactness of computer
arithmetic.

<<example-iv-linearity>>=
linearity <- as.vector(abs(nulls.sat) < all.equal.default.tolerance)
catrec[! linearity, ]
@

This agrees with (part of) Table 2 in \citet{geyer-gdor}
and with Section~10.4 of the supplementary material for \citet{eck-geyer}.

Fit the LCM.
<<example-iv-glm-lcm>>=
gout.lcm <- glm(y ~ (.)^3, family = "poisson", data = catrec,
    subset = linearity)
summary(gout.lcm)
@

This agrees
with Section~10.5 of the supplementary material for \citet{eck-geyer}.

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-iv-glm-vcov>>=
v <- vcov(gout.lcm)
v <- v[! is.na(coefficients(gout.lcm)), ]
v <- v[ , ! is.na(coefficients(gout.lcm))]
range(eigen(v, symmetric = TRUE, only.values = TRUE)$values)
@

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}
and the tolerance.
<<example-iv-clean>>=
<<example-i-clean>>
@

\section{Example V}

\subsection{Fit Using R Function \texttt{glm}}

This is the sports example of Geyer
(see help for this dataset for source).
<<example-v-glm>>=
data(sports)
gout <- glm(cbind(wins, losses) ~ 0 + ., family = "binomial",
    data = sports, x = TRUE)
summary(gout)
@

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-v-glm-fish>>=
# extract model matrix, response vector, and offset vector
modmat <- gout$x
mf <- model.frame(gout)
resp <- model.response(mf)
n <- rowSums(resp)
y <- resp[ , "wins"]
offs <- model.offset(mf)

# reduce dimension of coefficient vector and model matrix
inies <- ! is.na(coefficients(gout))
modmat <- modmat[ , inies, drop = FALSE]
beta <- coefficients(gout)[inies]

# calculate minus log likelihood and two derivatives
mlogl <- glmdr:::make.mlogl(modmat, resp, offs, "binomial")
mout <- mlogl(beta)
@

Check hessian is correct.
<<example-v-check-fish>>=
theta.glm <- predict(gout)
p.glm <- invlogit(theta.glm)
q.glm <- invlogit(- theta.glm)
fish.glm <- t(modmat) %*% diag(n * p.glm * q.glm) %*% modmat
all.equal(mout$hessian, fish.glm, check.attributes = FALSE)
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-v-fish-eigen>>=
<<example-i-fish-eigen>>
@

\subsection{Newton Step}

Find Newton step.
<<example-v-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

This looks very close to the generalized direction of recession (GDOR)
given by \citet[Table~4]{geyer-gdor} if we allow for the fact that
the last coefficient (for the hogs) is being constrained to be zero.

Map to saturated model canonical parameter space.
<<example-v-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
team.names <- colnames(gout$x)
team.plus <- apply(gout$x, 1, function(x) team.names[x == 1])
team.minus <- apply(gout$x, 1, function(x) team.names[x == - 1])
data.frame(plus = team.plus, minus = team.minus,
    step = zapsmall(newt.step.sat))
newt.step.sat
@

This is not exactly a DOR.
<<example-v-dor-check>>=
is.DOR.binomial <- function(theta)
    all(ifelse(theta < 0, y == 0, ifelse(theta > 0, y == n, TRUE)))
is.DOR.binomial(newt.step.sat)
@

But it is approximately.
<<example-v-newt-sat-zap>>=
is.DOR.binomial(zapsmall(newt.step.sat))
@

But this is not much help.

\subsection{Line Search}

So we do a line search in the line along the Newton step.
<<example-v-line-search>>=
linesearchfun <- function(s) {
    mout <- mlogl(beta + s * newt.step)
    sum(mout$gradient * newt.step)
}
foo <- Vectorize(linesearchfun)(0:32)
foo
@

And as in Example~IV we take the path
up to (but not including) the first positive result or the first negative
difference of results.
<<example-v-beta-too>>=
foo.good <- foo < 0 & c(Inf, diff(foo)) > 0
ifoo <- min(which(! foo.good)) - 1
ifoo
beta.too <- beta + ifoo * newt.step
@

And redo the Fisher information calculation.
<<example-v-fish-lcm>>=
<<example-ii-fish-lcm>>
@

<<example-v-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

\subsection{Limiting Conditional Model}
We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-v-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
# is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
# nulls

nulls.sat <- modmat %*% nulls
dim(nulls.sat)
linearity <- apply(abs(nulls.sat), 1,
    function(x) max(x) < all.equal.default.tolerance)
data.frame(plus = team.plus, minus = team.minus,
    linearity)
@

Here we have \emph{not} calculated the linearity exactly,
so we have to worry about inexactness of computer
arithmetic.
But this is the correct linearity for this problem.

Fit the LCM.
<<example-v-glm-lcm>>=
gout.lcm <- glm(cbind(wins, losses) ~ 0 + ., family = "binomial",
    data = sports, subset = linearity)
summary(gout.lcm)
@

This agrees with (part of) Table 4 in \citet{geyer-gdor}
and with Section~8.4 of the supplementary material for \citet{eck-geyer}.

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-v-glm-vcov>>=
<<example-iv-glm-vcov>>
@

\subsection{Clean Up}

Clean R global environment.
<<example-v-clean>>=
<<example-i-clean>>
@

\section{Example VI}

\subsection{Fit Using R Function \texttt{glm}}

This is the big categorical data example of Eck
(see help for this dataset for source).
<<example-vi-glm>>=
data(bigcategorical)
gout <- glm(y ~ (.)^4, family = "poisson", data = bigcategorical,
    x = TRUE)
# summary(gout) # too much printout
@

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-vi-glm-fish>>=
<<example-iv-glm-fish>>
@

Check hessian is correct.
<<example-vi-check-fish>>=
<<example-iv-check-fish>>
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-vi-fish-eigen-make>>=
eout <- eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)
@

Figure~\ref{fig:eigen} is made by the following code
<<example-vi-fish-eigen-plot,eval=FALSE>>=
plot(eout$values, ylab = "eigenvalues", log = "y")
abline(h = all.equal.default.tolerance)
@
\begin{figure}
\begin{center}
<<example-vi-fish-eigen,echo=FALSE>>=
<<example-vi-fish-eigen-plot>>
@
\end{center}
\caption{Eigenvalues of Fisher information matrix for MLE produced by
R function \texttt{glm}.  Solid line is default tolerance of
R function \texttt{all.equal}.}
\label{fig:eigen}
\end{figure}
This figure shows that the default tolerance for R function \texttt{all.equal}
doesn't do the right thing for this example.  As we said before, one could
use a different tolerance, but what?  It order to choose the tolerance that
gives the right answer for a problem, you already need to know the right answer!

\subsection{Newton Step}

Find Newton step.
<<example-vi-newt>>=
newt.step <- solve(mout$hessian, - mout$gradient)
# newt.step # too big to show
@

\subsection{Direction of Recession}

Could the Newton direction possibly be a DOR?

Map to saturated model canonical parameter space.
<<example-vi-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
# newt.step.sat # too big to show
@

This is not exactly a DOR.
<<example-vi-dor-check>>=
<<example-iv-dor-check>>
@

But it is approximately.
<<example-vi-newt-sat-zap>>=
<<example-iv-newt-sat-zap>>
@

But this is not much help. 

\subsection{Line Search}

So we do a line search in the line along the Newton step.
<<example-vi-line-search>>=
<<example-iv-line-search>>
@

And as in Example~IV we take the path
up to (but not including) the first positive result or the first negative
difference of results.
<<example-vi-beta-too>>=
<<example-iv-beta-too>>
@

And redo the Fisher information calculation.
<<example-vi-fish-lcm>>=
mout <- mlogl(beta.too)
eout <- eigen(mout$hessian, symmetric = TRUE)
sum(eout$values < all.equal.default.tolerance)
@

<<example-vi-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

There are too many eigenvalues to show, but at least we have the
right dimension of the null space of the Fisher information matrix
for the LCM, according to Section~11.4 of the supplementary material
for \citet{eck-geyer}.

Since some eigenvalues are negative, we cannot repeat Figure~\ref{fig:eigen}.
Instead we look at what we now hope is a large eigenvalue gap that contains
the default tolerance
<<example-vi-gap>>=
min(eout$values[eout$values >= all.equal.default.tolerance])
max(eout$values[eout$values < all.equal.default.tolerance])
@

\subsection{Limiting Conditional Model}

We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-vi-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
# is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
# nulls

nulls.sat <- modmat %*% nulls
dim(nulls.sat)
linearity <- apply(abs(nulls.sat), 1, max) < all.equal.default.tolerance
sum(! linearity)
subset(bigcategorical, ! linearity)
@
This is the correct linearity, agreeing with Sections~11.5 and~11.7
of the supplementary material for \citet{eck-geyer}.

Fit the LCM.
<<example-vi-glm-lcm>>=
gout.lcm <- glm(y ~ (.)^4, family = "poisson",
    data = bigcategorical, subset = linearity)
# summary(gout.lcm) # too big to show
@

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-vi-glm-vcov>>=
<<example-iv-glm-vcov>>
@

\section{Discussion}

So the trick of ``polishing'' the result of R function \texttt{glm}
is to do one (or perhaps more, but we only needed one for all of our
examples) Newton steps safeguarded by a line search.
\begin{itemize}
\item If the Newton direction is actually a DOR, then we could follow
    the line search all the way to infinity if the computer calculated
    with real real numbers.  But since R cannot do symbolic limits,
    all we can do is take a large multiple of the Newton step.
\item If the Newton direction is not exactly a DOR, but DOR exist,
    (so the MLE does not exist in the conventional sense),
    then we can follow the line search a long way but not all the way
    to infinity (our line search step will be many times what the
    Newton step would have been.

    A good strategy for this case seems to be to follow the line
    search until the gradient along the line search goes positive
    or the difference of gradients along the line search goes negative.
\item If no DOR exist (so the MLE does exist in the conventional sense
    and the result of R function \texttt{glm} is close to correct),
    then nothing bad happens. We just do very close to the Newton
    step and slightly improve the accuracy of the MLE.
\end{itemize}

After this operation, it is much easier to discover null eigenvalues
of the Fisher information matrix.  In all the examples the largest
calculated eigenvalue that we decided was truly zero was
\Sexpr{max(.polish.max.eigenzero)} which occurred
in Example~\Sexpr{library(utils); as.roman(which(.polish.max.eigenzero == max(.polish.max.eigenzero)))}.

This is quite a bit below the cutoff we used
<<discuss-cutoff>>=
all.equal.default.tolerance
@

Presumably we could use
<<discuss-cutoff-too>>=
(.Machine$double.eps)^(3 / 4)
@
\noindent
or something of the sort.

We can special case the case of no eigenvalues zero (the MLE exists in
the conventional sense) and all eigenvalues zero
(the MLE in the Barndorff-Nielsen completion is completely degenerate).
In either case the LCM is trivial and need not be fitted.
In the first case, R function \texttt{glm} has already done the work.
In the second case, there is no work to do, because a completely degenerate
model has no identifiable parameters.

\begin{thebibliography}{}

\bibitem[Eck and Geyer(submitted)]{eck-geyer}
Eck, D.~J. and Geyer, C.~J. (submitted).
\newblock Computationally efficient likelihood inference
    in exponential families when the maximum likelihood estimator
    does not exist.
\newblock \url{https://arxiv.org/abs/1803.11240}.

\bibitem[Geyer(2009)]{geyer-gdor}
Geyer, Charles J. (2009).
\newblock Likelihood inference in exponential families and directions
    of recession.
\newblock \emph{Electronic Journal of Statistics}, \textbf{3}, 259--289.

\end{thebibliography}


\end{document}

